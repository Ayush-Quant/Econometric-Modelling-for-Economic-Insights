# -*- coding: utf-8 -*-
"""ECOTRIX FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zMMUUp3EnESdh5e5cFIH7uEsFtV9kY8o

**IMPORTING MAIN LIBRARIES**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files
uploaded = files.upload()



"""**DATA IMPUTATION**"""

import io
df = pd.read_excel(io.BytesIO(uploaded['ecotrix final.xlsx']))
df.drop('Unnamed: 2', axis=1, inplace=True)

print(df)
df.head(10)

from sklearn.impute import KNNImputer
for col in df.columns:
    if df[col].dtype == 'object':  # Assuming object columns need conversion
        df[col] = pd.to_numeric(df[col].str.replace(',', ''), errors='coerce')
    else:
        df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric

print("Before Imputation:")
print(df.head())
knn_imputer = KNNImputer(n_neighbors=3)
data_imputed = knn_imputer.fit_transform(df)
data_imputed_df = pd.DataFrame(data_imputed, columns=df.columns)

print("\nAfter Imputation:")
print(data_imputed_df.head())



"""**DATA EXPLORATION**"""

sns.boxplot(x='Account, female (% age 15+)', data=data_imputed_df)

plt.title('Box Plot for inclusion of female')
plt.xlabel('female accounts')
plt.show()

sns.boxplot(x='Account, in labor force (% age 15+)', data=data_imputed_df)

plt.title('Box Plot for inclusion of labor force')
plt.xlabel('labor accounts')
plt.show()

for col in data_imputed_df.columns:
    # Check if the column dtype is object (likely string) before applying .str
    if df[col].dtype == 'object':
        df[col] = pd.to_numeric(df[col].str.replace(',', ''), errors='coerce')
    else:
        # If not an object dtype, it's likely already numeric or a mixed type
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Now calculate the correlation matrix
corr_matrix = data_imputed_df.corr()
plt.figure(figsize=(15,10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

import statsmodels.api as sm
import statsmodels.formula.api as smf


# Define the dependent and independent variables
# Example: Assuming the dependent variable is 'Y' and independent variables are 'X1', 'X2', and 'X3'
X = data_imputed_df[['Account, female (% age 15+)', 'Account, in labor force (% age 15+)', 'Account, income, poorest 40% (% ages 15+)','Account, income, richest 60% (% ages 15+)','Account, male (% age 15+)','Account, out of labor force (% age 15+)','Account, primary education or less (% ages 15+)','Account, secondary education or more (% ages 15+)']]
y = data_imputed_df['GNI(Y)']

X = X.replace([np.inf, -np.inf], np.nan).dropna() # Replace inf with NaN and drop rows with NaN
y = y.replace([np.inf, -np.inf], np.nan).dropna() # Replace inf with NaN and drop rows with NaN

# Ensure X and y have the same index after dropping rows
# This is crucial to avoid errors when fitting the model
common_index = X.index.intersection(y.index)
X = X.loc[common_index]
y = y.loc[common_index]

X = sm.add_constant(X)

model = sm.OLS(y, X).fit()

residuals = model.resid

independent_vars = ['Account, female (% age 15+)', 'Account, in labor force (% age 15+)', 'Account, income, poorest 40% (% ages 15+)','Account, income, richest 60% (% ages 15+)','Account, male (% age 15+)','Account, out of labor force (% age 15+)','Account, primary education or less (% ages 15+)','Account, secondary education or more (% ages 15+)'] # List of independent variables
plt.figure(figsize=(15, 5))

for i, var in enumerate(independent_vars, 1):
    partial_residual = residuals + model.params[var] * X[var]

    # Create a subplot for each independent variable
    plt.subplot(1, len(independent_vars), i)

    # Scatter plot of partial residuals vs. the independent variable
    sns.scatterplot(x=X[var], y=partial_residual)

    # Add a regression line
    sns.regplot(x=X[var], y=partial_residual, scatter=False, color='red')

    # Add title and labels
    plt.title(f'Partial Residual Plot for {var}')
    plt.xlabel(var)
    plt.ylabel('Partial Residuals')

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

X = sm.add_constant(X)  # Add intercept

# Fit the OLS model
model = sm.OLS(y, X).fit()

for col in X.columns[1:]:  # Skip the constant
    fig = sm.graphics.plot_ccpr(model, col)
    plt.show()



from scipy.stats import boxcox
data_cleaned = data_imputed_df[data_imputed_df['Account, secondary education or more (% ages 15+)'] > 0]

transformed_data, lambda_value = boxcox(data_cleaned['Account, secondary education or more (% ages 15+)'])
data_cleaned['Account, secondary education or more (% ages 15+)'] = transformed_data

print("Lambda value used for Box-Cox Transformation:", lambda_value)
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.histplot(data_cleaned['Account, secondary education or more (% ages 15+)'], kde=True, color='blue')
plt.title('Original Data Distribution')

plt.subplot(1, 2, 2)
sns.histplot(data_cleaned['Account, secondary education or more (% ages 15+)'], kde=True, color='green')
plt.title('Box-Cox Transformed Data Distribution')

plt.tight_layout()
plt.show()

from statsmodels.stats.diagnostic import het_breuschpagan
X = sm.add_constant(X)
model = sm.OLS(y, X).fit()


# Perform the Breusch-Pagan test
test_stat, p_value, _, _ = het_breuschpagan(model.resid, X)

# Output the results
print(f"Breusch-Pagan Test Statistic: {test_stat}")
print(f"P-Value: {p_value}")

# Interpretation
if p_value < 0.05:
    print("Reject the null hypothesis: Heteroscedasticity is present.")
else:
    print("Fail to reject the null hypothesis: No evidence of heteroscedasticity.")

import scipy.stats as stats
residuals = model.resid
standardized_residuals = (residuals - residuals.mean()) / residuals.std()
plt.figure(figsize=(6, 6))
stats.probplot(standardized_residuals, dist="norm", plot=plt)
plt.title('QQ Plot of Standardized Residuals vs. Standard Normal')
plt.show()

from scipy.stats import shapiro
X = data_cleaned[['Account, female (% age 15+)', 'Account, in labor force (% age 15+)', 'Account, income, poorest 40% (% ages 15+)','Account, income, richest 60% (% ages 15+)','Account, male (% age 15+)','Account, out of labor force (% age 15+)','Account, primary education or less (% ages 15+)','Account, secondary education or more (% ages 15+)']]
y = data_cleaned['GNI(Y)']

X = X.replace([np.inf, -np.inf], np.nan).dropna()
y = y.replace([np.inf, -np.inf], np.nan).dropna()

common_index = X.index.intersection(y.index)
X = X.loc[common_index]
y = y.loc[common_index]

X = sm.add_constant(X)

model = sm.OLS(y, X).fit()

residuals = model.resid

statistic, p_value = shapiro(residuals)
print(f"Shapiro-Wilk Test Statistic: {statistic}")
print(f"P-value: {p_value}")


alpha = 0.05
if p_value > alpha:
    print("The residuals seem to be normally distributed (fail to reject H0)")
else:
    print("The residuals do not follow a normal distribution (reject H0)")

model = sm.OLS(y, X).fit()

residuals = model.resid

residuals_log = np.log(residuals + 1)

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.hist(residuals, bins=30, edgecolor='black')
plt.title('Original Residuals')

plt.subplot(1, 2, 2)
plt.hist(residuals_log, bins=30, edgecolor='black')
plt.title('Log-Transformed Residuals')

plt.show()

statistic, p_value = shapiro(residuals_log)

print(f"Shapiro-Wilk Test Statistic: {statistic}")
print(f"P-value: {p_value}")

# Interpret the result
alpha = 0.05
if p_value > alpha:
    print("The log-transformed residuals seem to be normally distributed (fail to reject H0)")
else:
    print("The log-transformed residuals do not follow a normal distribution (reject H0)")

model = sm.OLS(y, X).fit()

residuals = model.resid

if (residuals <= 0).any():
    print("Residuals contain zero or negative values, Box-Cox transformation cannot be applied.")
else:

    transformed_residuals, lambda_ = boxcox(residuals)

    plt.figure(figsize=(12, 6))


    plt.subplot(1, 2, 1)
    plt.hist(residuals, bins=30, edgecolor='black')
    plt.title('Original Residuals')


    plt.subplot(1, 2, 2)
    plt.hist(transformed_residuals, bins=30, edgecolor='black')
    plt.title('Box-Cox Transformed Residuals')

    plt.show()


    statistic, p_value = shapiro(transformed_residuals)

    # Print the test statistic and p-value
    print(f"Shapiro-Wilk Test Statistic: {statistic}")
    print(f"P-value: {p_value}")

    # Interpret the result
    alpha = 0.05  # Significance level
    if p_value > alpha:
        print("The Box-Cox transformed residuals seem to be normally distributed (fail to reject H0)")
    else:
        print("The Box-Cox transformed residuals do not follow a normal distribution (reject H0)")

from sklearn.preprocessing import PowerTransformer
model = sm.OLS(y, X).fit()
residuals = model.resid

scaler = PowerTransformer(method='yeo-johnson')
residuals_yeo_johnson = scaler.fit_transform(residuals.values.reshape(-1, 1))  # Reshape to 2D array

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(residuals, bins=30, edgecolor='black')
plt.title('Original Residuals')


plt.subplot(1, 2, 2)
plt.hist(residuals_yeo_johnson, bins=30, edgecolor='black')
plt.title('Yeo-Johnson Transformed Residuals')

plt.show()

statistic, p_value = shapiro(residuals_yeo_johnson)

print(f"Shapiro-Wilk Test Statistic: {statistic}")
print(f"P-value: {p_value}")

# Interpret the result
alpha = 0.05  # Significance level
if p_value > alpha:
    print("The Yeo-Johnson transformed residuals seem to be normally distributed (fail to reject H0)")
else:
    print("The Yeo-Johnson transformed residuals do not follow a normal distribution (reject H0)")

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
data_cleaned = data_imputed_df.copy()  # Ensure `data_cleaned` is defined
response_variable_scaled = scaler.fit_transform(data_cleaned[['GNI(Y)']])  # Scale the column

data_cleaned['GNI(Y)_scaled'] = response_variable_scaled  # Assign the scaled values to a new column
data_cleaned.head()

from scipy.stats import norm
from scipy import stats
n_iterations = 1000  # Number of bootstrap samples
n_size = len(residuals_yeo_johnson)  # Size of each sample (same size as original data)

# Generate bootstrap samples and compute their means (or any other statistic)
bootstrap_means = np.zeros(n_iterations)

for i in range(n_iterations):
    sample = np.random.choice(residuals_yeo_johnson.flatten(), size=n_size, replace=True)   # Resampling with replacement
    bootstrap_means[i] = np.mean(sample)

# Plot the bootstrap distribution of the mean
plt.hist(bootstrap_means, bins=50, edgecolor='black', alpha=0.7)
plt.title('Bootstrap Distribution of the Mean')
plt.xlabel('Mean')
plt.ylabel('Frequency')
plt.show()

# Check normality of the bootstrap distribution
statistic, p_value = stats.shapiro(bootstrap_means)
print(f"Shapiro-Wilk Test on Bootstrap Distribution: Statistic = {statistic}, p-value = {p_value}")

GNI = data_imputed_df['GNI(Y)']

# Specify the response variable and predictors
response_variable = 'GNI(Y)'
predictors = data_imputed_df.columns.difference([response_variable])

# Function to perform bootstrapping on the regression model
def bootstrap_model(data_imputed_df, response_variable, predictors, num_samples):
    """Bootstrap the regression model and return the coefficients."""
    bootstrapped_coefs = []
    n = len(data_imputed_df)

    for _ in range(num_samples):
        # Resample the dataset with replacement
        sample = data_imputed_df.sample(n=n, replace=True)

        # Fit the model to the bootstrap sample
        X = sample[predictors]
        X = sm.add_constant(X)  # Adds a constant term to the predictor
        y = sample[response_variable]

        model = sm.OLS(y, X).fit()
        bootstrapped_coefs.append(model.params)

    return pd.DataFrame(bootstrapped_coefs)

# Set parameters for bootstrapping
num_samples = 1000  # Number of bootstrap samples

# Perform bootstrapping
bootstrapped_results = bootstrap_model(data_imputed_df, response_variable,
predictors, num_samples)

# Calculate the mean and confidence intervals for the coefficients
mean_coefs = bootstrapped_results.mean()
lower_bound = bootstrapped_results.quantile(0.025)
upper_bound = bootstrapped_results.quantile(0.975)

# Display the results
print("Bootstrapped Coefficients with 95% Confidence Intervals:")
results_df = pd.DataFrame({
    'Mean Coefficient': mean_coefs,
    'Lower Bound': lower_bound,
    'Upper Bound': upper_bound
})
print(results_df)

# Optional: Visualize the distribution of bootstrapped coefficients

plt.figure(figsize=(10, 5))
plt.hist(bootstrapped_results.iloc[:, 1], bins=30, edgecolor='k',
alpha=0.7)  # Change index for different predictors
plt.axvline(mean_coefs[1], color='red', linestyle='dashed',
linewidth=2, label='Mean Coefficient')
plt.axvline(lower_bound[1], color='blue', linestyle='dashed',
linewidth=2, label='95% CI Lower Bound')
plt.axvline(upper_bound[1], color='blue', linestyle='dashed',
linewidth=2, label='95% CI Upper Bound')
plt.title('Distribution of Bootstrapped Coefficients for Predictor')
plt.xlabel('Coefficient Value')
plt.ylabel('Frequency')
plt.legend()
plt.show()

print(model.summary())

const = 6.182349e+06
beta_female = -9.509084e+04
beta_labor_force = 3.594368e+05
beta_income_poorest = 2.242415e+05
beta_income_richest = -3.764125e+05
beta_male = -1.390744e+05
beta_out_of_labor = -7.854734e+04
beta_primary_education = 1.971572e+05
beta_secondary_education = -9.566448e+04
y_pred = (
        const
        + beta_female * data_imputed_df['Account, female (% age 15+)']
        + beta_labor_force * data_imputed_df['Account, in labor force (% age 15+)']
        + beta_income_poorest * data_imputed_df['Account, income, poorest 40% (% ages 15+)']
        + beta_income_richest * data_imputed_df['Account, income, richest 60% (% ages 15+)']
        + beta_male * data_imputed_df['Account, male (% age 15+)']
        + beta_out_of_labor * data_imputed_df['Account, out of labor force (% age 15+)']
        + beta_primary_education * data_imputed_df['Account, primary education or less (% ages 15+)']
        + beta_secondary_education * data_imputed_df['Account, secondary education or more (% ages 15+)']
    )

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(data_imputed_df['GNI(Y)'], y_pred)
print(f'Mean Squared Error: {mse}')

"""**MODEL SELECTION AND REGRESSOR ELIMINATION**"""

model = sm.OLS(y, X).fit()

RSS_full = np.sum(model.resid**2)

n = len(y)
p = X.shape[1]

sigma_squared = RSS_full / (n - 2 * p)
cp = (RSS_full / sigma_squared) - (n - 2 * p)

print(f"Mallow's Cp: {cp};")

from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


lasso = Lasso(alpha=0.1)


lasso.fit(X_train_scaled, y_train)


y_pred = lasso.predict(X_test_scaled)


mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Lasso Coefficients: {lasso.coef_}')
print(f'Intercept: {lasso.intercept_}')
print(f'Mean Squared Error: {mse}')
print(f'R^2: {r2}')


plt.figure(figsize=(8, 6))
plt.bar(range(len(lasso.coef_)), lasso.coef_)
plt.xticks(range(len(lasso.coef_)), X.columns, rotation=90)
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.title('Lasso Regression Coefficients')
plt.show()


alphas = np.logspace(-4, 1, 50)
mse_alphas = []

for alpha in alphas:
    lasso = Lasso(alpha=alpha)
    lasso.fit(X_train_scaled, y_train)
    y_pred = lasso.predict(X_test_scaled)
    mse_alphas.append(mean_squared_error(y_test, y_pred))


plt.figure(figsize=(8, 6))
plt.plot(alphas, mse_alphas, label='MSE for different alpha values')
plt.xscale('log')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Mean Squared Error')
plt.title('Lasso Regularization Tuning')
plt.show()

print(model.summary())

X = df2[['Account, female (% age 15+)', 'Account, in labor force (% age 15+)', 'Account, income, poorest 40% (% ages 15+)','Account, income, richest 60% (% ages 15+)','Account, male (% age 15+)','Account, out of labor force (% age 15+)','Account, primary education or less (% ages 15+)','Account, secondary education or more (% ages 15+)']]
X = sm.add_constant(X)  # Add intercept
y = df2["GNI(Y)"]
def stepwise_regression(X, y, p_value_threshold=0.05):
    while True:

        model = sm.OLS(y, X).fit()
        print(model.summary())

        p_values = model.pvalues
        max_p_value = p_values.max()
        max_p_variable = p_values.idxmax()


        if max_p_value > p_value_threshold:
            print(f"\nDropping '{max_p_variable}' with p-value {max_p_value}\n")
            X = X.drop(columns=[max_p_variable])
        else:

            print("\nFinal Model Selected\n")
            break

    return model


final_model = stepwise_regression(X, y)


print(final_model.summary())

"""**OUTLIERS AND LEVERAGES**"""

model = sm.OLS(y, X).fit()

influence = model.get_influence()
leverage = influence.hat_matrix_diag
standardized_residuals = influence.resid_studentized_internal


plt.figure(figsize=(8, 6))
plt.scatter(leverage, standardized_residuals, edgecolors="k", alpha=0.7)
plt.axhline(y=0, color="red", linestyle="--", linewidth=1)
plt.axhline(y=2, color="blue", linestyle="--", linewidth=1)
plt.axhline(y=-2, color="blue", linestyle="--", linewidth=1)
plt.xlabel("Leverage")
plt.ylabel("Standardized Residuals")
plt.title("Leverage vs. Standardized Residuals Plot")
plt.grid(alpha=0.3)
plt.show()

model = sm.OLS(y, X)
results = model.fit()


influence = results.get_influence()

cooks_d = influence.cooks_distance[0]

plt.figure(figsize=(8,6))
markerline, stemlines, baseline = plt.stem(np.arange(len(cooks_d)), cooks_d, markerfmt="o")
plt.setp(stemlines, linestyle="-", color="black", linewidth=0.5)
plt.title("Cook's Distance")
plt.xlabel("Index")
plt.ylabel("Cook's Distance")
plt.show()

threshold = 4 / len(df)
outliers = np.where(cooks_d > threshold)[0]

print("Outliers detected at indices:", outliers)
print("Cook's distance values:", cooks_d[outliers])

df1 = data_imputed_df.drop([10 ,103,125])
df1.head()

model = sm.OLS(y, X).fit()

influence = model.get_influence()
leverage = influence.hat_matrix_diag

average_leverage = 2 * np.mean(leverage)
high_leverage_points = np.where(leverage > average_leverage)[0]

print("Leverage values:", leverage)
print("High leverage points (indices):", high_leverage_points)

print("Rows with high leverage points:")
print(df.iloc[high_leverage_points])

df2 = data_imputed_df.drop([ 2 ,  3 , 57 , 59 ,  60 , 70 , 82 , 90 , 111 , 112 , 115 , 146 , 153])
df2.head()

"""**ANOVA TESTING**"""

null_model = sm.OLS(df2['GNI(Y)'], sm.add_constant(np.ones(len(df2)))).fit()

unrestricted_model = sm.OLS(df2['GNI(Y)'], sm.add_constant(df2[['Account, in labor force (% age 15+)', 'Account, income, richest 60% (% ages 15+)', 'Account, primary education or less (% ages 15+)']])).fit()
# Perform ANOVA
# Use the correct function call for anova_lm
anova_results = sm.stats.anova_lm(null_model, unrestricted_model)

# Display the ANOVA results
print(anova_results)

"""**MULTICOLLINEARITY**"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
X = df2[['Account, in labor force (% age 15+)', 'Account, income, richest 60% (% ages 15+)', 'Account, primary education or less (% ages 15+)']]
y = df2['GNI(Y)']  # Assuming df2 contains the GNI(Y) column

X = add_constant(X)

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = np.sqrt([variance_inflation_factor(X.values, i) for i in range(X.shape[1])])

# Display the VIF results
print(vif_data)

influence_less = data_cleaned.drop([ 6 ,9, 11, 36 ,46 ,69 ,74 ,85 ,89 ,93])
influence_less.head()

rig_model2 = sm.OLS(data_cleaned['GNI(Y)_scaled'], X).fit()
print(rig_model2.summary())

shapiro_test = shapiro(rig_model2.resid)
print(shapiro_test)

import statsmodels.api as sm
from scipy.stats import kstest
kstest(rig_model2.resid,'norm')

new_data = data_cleaned.drop(['Account, male (% age 15+)'],axis=1)

rig_model3 = sm.OLS(new_data['GNI(Y)_scaled'], new_data.drop(['GNI(Y)_scaled','GNI(Y)'],axis=1)).fit()
print(rig_model3.summary())

kstest(rig_model3.resid,'norm')

shapiro_test = shapiro(rig_model3.resid)
print(shapiro_test)

